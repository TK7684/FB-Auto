"""
Gemini AI Service for Response Generation.

This module integrates with Google's Gemini API to generate
Thai language responses for customer inquiries about D Plus Skin products.
"""

import google.generativeai as genai
from typing import Optional, List, Dict, Any
from loguru import logger
import re
import json
from datetime import datetime
from pathlib import Path

from config.settings import settings
from config.constants import (
    SYSTEM_PROMPT,
    COMMENT_REPLY_PROMPT,
    MELASMA_SPECIAL_PROMPT,
    FALLBACK_RESPONSE,
    MELASMA_KEYWORDS,
    ACNE_KEYWORDS,
    DRY_SKIN_KEYWORDS,
    WRINKLE_KEYWORDS,
    SUPPLEMENT_KEYWORDS,
    SUPPLEMENT_KEYWORDS,
    PURCHASE_INTENT_KEYWORDS,
    SOCIAL_KEYWORDS,
    SOCIAL_SYSTEM_PROMPT
)
from services.memory_service import get_memory_service


# Load CTA configuration
def _load_cta_config() -> Dict[str, Any]:
    """Load product CTA configuration from JSON file."""
    cta_path = Path("data/products_cta.json")
    if cta_path.exists():
        try:
            with open(cta_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading CTA config: {e}")
    return {"categories": {}, "purchase_intent_keywords": [], "default_line_id": "@dplusskin"}

CTA_CONFIG = _load_cta_config()


class GeminiService:
    """
    Service for generating AI responses using Gemini API.

    Features:
    - Thai language support
    - Special handling for "à¸à¹‰à¸²" (melasma) queries
    - Context-aware responses based on product database
    - Fallback responses when API fails
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize Gemini service.

        Args:
            api_key: Gemini API key (uses settings if not provided)
        """
        self.api_key = api_key or settings.gemini_api_key
        self.model_name = settings.gemini_model
        self.memory_service = get_memory_service()

        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Gemini service initialized with model: {self.model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini: {e}")
            self.model = None

    def _is_melasma_query(self, text: str) -> bool:
        """
        Check if the query is about melasma (à¸à¹‰à¸²).

        Args:
            text: Query text

        Returns:
            True if query contains melasma keywords
        """
        return any(keyword in text for keyword in MELASMA_KEYWORDS)

    def _is_purchase_intent(self, text: str) -> bool:
        """
        Check if the query shows purchase intent.

        Args:
            text: Query text

        Returns:
            True if query shows intent to buy
        """
        return any(keyword in text for keyword in PURCHASE_INTENT_KEYWORDS)

    def _is_social_comment(self, text: str) -> bool:
        """
        Check if the query is social/chit-chat.
        """
        return any(keyword in text for keyword in SOCIAL_KEYWORDS)

    def _detect_skin_category(self, text: str) -> str:
        """
        Detect which skin problem category the query is about.

        Args:
            text: Query text (could be comment or post caption)

        Returns:
            Category name (à¸ªà¸´à¸§, à¸à¹‰à¸², à¸œà¸´à¸§à¹à¸«à¹‰à¸‡, à¸£à¸´à¹‰à¸§à¸£à¸­à¸¢, à¸­à¸²à¸«à¸²à¸£à¹€à¸ªà¸£à¸´à¸¡, or à¸—à¸±à¹ˆà¸§à¹„à¸›)
        """
        text_lower = text.lower()
        if any(kw in text for kw in ACNE_KEYWORDS):
            return "à¸ªà¸´à¸§"
        elif any(kw in text for kw in MELASMA_KEYWORDS):
            return "à¸à¹‰à¸²"
        elif any(kw in text for kw in SUPPLEMENT_KEYWORDS) or any(kw in text_lower for kw in SUPPLEMENT_KEYWORDS):
            return "à¸­à¸²à¸«à¸²à¸£à¹€à¸ªà¸£à¸´à¸¡"
        elif any(kw in text for kw in DRY_SKIN_KEYWORDS):
            return "à¸œà¸´à¸§à¹à¸«à¹‰à¸‡"
        elif any(kw in text for kw in WRINKLE_KEYWORDS):
            return "à¸£à¸´à¹‰à¸§à¸£à¸­à¸¢"
        return "à¸—à¸±à¹ˆà¸§à¹„à¸›"

    def _get_cta_for_category(self, category: str, post_caption: str = "", comment_text: str = "") -> str:
        """
        Get the CTA text and link for a given category with tier-based matching.

        Priority:
        1. Check post caption for exact product name
        2. Check comment/caption for tier keywords
        3. Fall back to default CTA for category

        Args:
            category: Skin problem category
            post_caption: Original post caption (primary source)
            comment_text: User's comment text

        Returns:
            CTA text with link
        """
        cat_config = CTA_CONFIG.get("categories", {}).get(category, {})
        if not cat_config:
            cat_config = CTA_CONFIG.get("categories", {}).get("à¸—à¸±à¹ˆà¸§à¹„à¸›", {})

        link = cat_config.get("link", "https://line.me/ti/p/@dplusskin")
        emoji = cat_config.get("emoji", "ðŸ’•")

        # Check for tier-based matching
        tiers = cat_config.get("tiers", {})
        combined_text = f"{post_caption} {comment_text}".lower()

        if tiers:
            # Check high tier keywords first (post caption priority)
            high_tier = tiers.get("high", {})
            high_keywords = high_tier.get("keywords", [])
            if any(kw.lower() in combined_text for kw in high_keywords):
                cta = high_tier.get("cta", cat_config.get("default_cta", "à¸ªà¸™à¹ƒà¸ˆà¸ªà¸´à¸™à¸„à¹‰à¸² à¸—à¸±à¸à¸¡à¸²à¹€à¸¥à¸¢à¸„à¹ˆà¸° ðŸ‘‰"))
                return f"{cta} {link} {emoji}"

            # Check low tier keywords
            low_tier = tiers.get("low", {})
            low_keywords = low_tier.get("keywords", [])
            if any(kw.lower() in combined_text for kw in low_keywords):
                cta = low_tier.get("cta", cat_config.get("default_cta", "à¸ªà¸™à¹ƒà¸ˆà¸ªà¸´à¸™à¸„à¹‰à¸² à¸—à¸±à¸à¸¡à¸²à¹€à¸¥à¸¢à¸„à¹ˆà¸° ðŸ‘‰"))
                return f"{cta} {link} {emoji}"

        # Fall back to default CTA
        cta = cat_config.get("default_cta", cat_config.get("cta", "à¸ªà¸™à¹ƒà¸ˆà¸ªà¸´à¸™à¸„à¹‰à¸² à¸—à¸±à¸à¸¡à¸²à¹€à¸¥à¸¢à¸„à¹ˆà¸° ðŸ‘‰"))
        return f"{cta} {link} {emoji}"


    def _build_prompt(
        self,
        user_question: str,
        context: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        Build the complete prompt for Gemini.

        Args:
            user_question: User's question
            context: Product/relevant information context (may include post caption)
            conversation_history: Previous messages for context

        Returns:
            Complete prompt string
        """
        # Extract post caption from context if available
        post_caption = ""
        if "à¸šà¸£à¸´à¸šà¸—à¹‚à¸žà¸ªà¸•à¹Œ:" in context:
            lines = context.split("\n")
            for line in lines:
                if line.startswith("à¸šà¸£à¸´à¸šà¸—à¹‚à¸žà¸ªà¸•à¹Œ:"):
                    post_caption = line.replace("à¸šà¸£à¸´à¸šà¸—à¹‚à¸žà¸ªà¸•à¹Œ:", "").strip()
                    break

        # Detect category from post caption + user question
        combined_text = f"{post_caption} {user_question}"
        category = self._detect_skin_category(combined_text)
        cta_text = self._get_cta_for_category(category, post_caption, user_question)

        # Check if this is a purchase intent
        is_purchase = self._is_purchase_intent(user_question)

        # Check if social/chit-chat
        if self._is_social_comment(user_question):
            # Use Social Prompt
            # Try to get a specific example from memory if available
            example = self.memory_service.get_random_example("social")
            prompt = SOCIAL_SYSTEM_PROMPT
            if example:
                prompt += f"\n\n## à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸ˆà¸£à¸´à¸‡à¸ˆà¸²à¸à¹à¸­à¸”à¸¡à¸´à¸™\nà¸¥à¸¹à¸à¸„à¹‰à¸²: {example['question']}\nà¹à¸­à¸”à¸¡à¸´à¸™: {example['answer']}"
            
            prompt += f"\n\n## à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¥à¸¹à¸à¸„à¹‰à¸²\n{user_question}\n\n## à¸•à¸­à¸šà¸à¸¥à¸±à¸š (à¸ªà¸±à¹‰à¸™à¹† à¸™à¹ˆà¸²à¸£à¸±à¸):"
            return prompt

        # Use COMMENT_REPLY_PROMPT for short, CTA-focused responses
        if post_caption or is_purchase:
            prompt = COMMENT_REPLY_PROMPT.format(
                post_caption=post_caption or "(à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸žà¸ªà¸•à¹Œ)",
                cta_text=cta_text,
                comment_text=user_question
            )
        else:
            # Use regular system prompt for general queries
            prompt = SYSTEM_PROMPT

            # Add special handling for melasma if needed
            if self._is_melasma_query(user_question):
                prompt += f"\n\n{MELASMA_SPECIAL_PROMPT}"

            # Add context
            prompt += f"\n\n## Context (à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸´à¸™à¸„à¹‰à¸²à¹à¸¥à¸°à¸šà¸£à¸´à¸šà¸—)\n{context}"
            
            # --- MEMORY INJECTION ---
            similar_memories = self.memory_service.find_similar(user_question, category="product")
            if similar_memories:
                prompt += "\n\n## à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¸•à¸­à¸šà¹ƒà¸™à¸­à¸”à¸µà¸• (à¹€à¸¥à¸µà¸¢à¸™à¹à¸šà¸šà¹‚à¸—à¸™à¸™à¸µà¹‰)\n"
                for mem in similar_memories:
                    prompt += f"- Q: {mem['question']}\n  A: {mem['answer']}\n"
            # ------------------------

            # Add current question
            prompt += f"\n\n## à¸„à¸³à¸–à¸²à¸¡à¸¥à¸¹à¸à¸„à¹‰à¸²\n{user_question}"

            # Add CTA instruction
            prompt += f"\n\nCTA à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰: {cta_text}"

            # Add response instruction
            prompt += "\n\n## à¸„à¸³à¸•à¸­à¸š (à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™à¹† 1-2 à¸šà¸£à¸£à¸—à¸±à¸” à¸žà¸£à¹‰à¸­à¸¡ CTA)\n"

        return prompt

    def _extract_product_mentions(self, response: str) -> List[str]:
        """
        Extract product names mentioned in the response.

        Args:
            response: Generated response text

        Returns:
            List of product names mentioned
        """
        # This is a simple implementation
        # In production, you might use regex patterns or NER
        products = []

        # Common product patterns
        patterns = [
            r"D Plus \w+",
            r"à¸‹à¸µà¸£à¸±à¹ˆà¸¡.*",
            r"à¸„à¸£à¸µà¸¡.*",
            r"à¹€à¸ˆà¸¥.*",
        ]

        for pattern in patterns:
            matches = re.findall(pattern, response)
            products.extend(matches)

        return list(set(products))

    def _validate_response(self, response: str) -> bool:
        """
        Validate the generated response.

        Args:
            response: Generated response text

        Returns:
            True if response is valid
        """
        if not response or len(response.strip()) < 10:
            logger.warning("Response too short or empty")
            return False

        # Check for Thai characters
        if not re.search(r'[\u0E00-\u0E7F]', response):
            logger.warning("Response doesn't contain Thai characters")

        # Check response length (should be reasonable)
        if len(response) > 2000:
            logger.warning(f"Response too long: {len(response)} characters")
            return False

        return True

    async def generate_response(
        self,
        user_question: str,
        context: str,
        conversation_history: Optional[List[Dict]] = None,
        max_retries: int = 2
    ) -> str:
        """
        Generate a response using Gemini AI.

        Args:
            user_question: User's question
            context: Product/relevant information
            conversation_history: Previous messages
            max_retries: Maximum retry attempts

        Returns:
            Generated response text
        """
        if not self.model:
            logger.error("Gemini model not initialized, using fallback")
            return self._get_fallback_response(user_question, context)

        # Build prompt
        prompt = self._build_prompt(user_question, context, conversation_history)

        # Log for debugging
        is_melasma = self._is_melasma_query(user_question)
        logger.info(
            f"Generating response for: {user_question[:50]}... "
            f"(melasma: {is_melasma})"
        )

        # Try to generate with retries
        for attempt in range(max_retries + 1):
            try:
                response = self.model.generate_content(prompt)

                if not response or not response.text:
                    logger.warning(f"Empty response from Gemini (attempt {attempt + 1})")
                    if attempt < max_retries:
                        continue
                    return self._get_fallback_response(user_question, context)

                # Validate response
                if self._validate_response(response.text):
                    logger.info(f"âœ“ Response generated: {len(response.text)} characters")
                    return response.text
                else:
                    logger.warning(f"Response validation failed (attempt {attempt + 1})")
                    if attempt < max_retries:
                        continue

            except Exception as e:
                logger.error(
                    f"Gemini API error (attempt {attempt + 1}/{max_retries + 1}): {e}"
                )

                if "quota" in str(e).lower() or "limit" in str(e).lower():
                    logger.error("Gemini quota exceeded, using fallback")
                    return self._get_fallback_response(user_question, context)

                if attempt < max_retries:
                    continue

        # All retries failed, use fallback
        logger.warning("All retries failed, using fallback response")
        return self._get_fallback_response(user_question, context)

    def _get_fallback_response(self, question: str, context: str) -> str:
        """
        Get a fallback response when AI generation fails.

        Args:
            question: User's question
            context: Product context

        Returns:
            Fallback response
        """
        # Try to provide a helpful response even without AI
        response = FALLBACK_RESPONSE.format(context=context)

        # Add personalization for melasma queries
        if self._is_melasma_query(question):
            response = (
                "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸° à¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸à¹‰à¸²à¹ƒà¸Šà¹ˆà¹„à¸«à¸¡à¸„à¸° ðŸŒ¸\n\n"
                "à¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸±à¸à¸«à¸²à¸à¹‰à¸² à¹€à¸£à¸²à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¸œà¸¥à¸´à¸•à¸ à¸±à¸“à¸‘à¹Œà¸—à¸µà¹ˆà¸¡à¸µà¸ªà¹ˆà¸§à¸™à¸œà¸ªà¸¡à¸‚à¸­à¸‡:\n"
                "- Vitamin C\n"
                "- Niacinamide\n"
                "- Tranexamic Acid\n\n"
                "à¹à¸¥à¸°à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸à¸„à¸·à¸­à¸•à¹‰à¸­à¸‡à¸—à¸²à¸„à¸£à¸µà¸¡à¸à¸±à¸™à¹à¸”à¸”à¸—à¸¸à¸à¸§à¸±à¸™à¸™à¸°à¸„à¸° â˜€ï¸\n\n"
                f"{context}\n\n"
                "à¸à¹‰à¸²à¹€à¸›à¹‡à¸™à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸­à¸”à¸—à¸™à¹ƒà¸Šà¹‰à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­à¸„à¹ˆà¸° à¸›à¸à¸•à¸´à¸ˆà¸°à¹€à¸«à¹‡à¸™à¸œà¸¥à¹ƒà¸™ 4-8 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ ðŸ’•"
            )

        return response

    async def generate_response_streaming(
        self,
        user_question: str,
        context: str,
        conversation_history: Optional[List[Dict]] = None
    ):
        """
        Generate a streaming response (for future use).

        Args:
            user_question: User's question
            context: Product context
            conversation_history: Previous messages

        Yields:
            Response chunks
        """
        if not self.model:
            yield self._get_fallback_response(user_question, context)
            return

        prompt = self._build_prompt(user_question, context, conversation_history)

        try:
            response = self.model.generate_content(prompt, stream=True)

            for chunk in response:
                if chunk.text:
                    yield chunk.text

        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield self._get_fallback_response(user_question, context)

    def test_connection(self) -> bool:
        """
        Test the Gemini API connection.

        Returns:
            True if connection successful
        """
        try:
            response = self.model.generate_content("à¸ªà¸§à¸±à¸ªà¸”à¸µ")
            if response and response.text:
                logger.info(f"âœ“ Gemini connection test successful: {response.text[:50]}")
                return True
        except Exception as e:
            logger.error(f"Gemini connection test failed: {e}")

        return False


# Singleton instance
_gemini_service: Optional[GeminiService] = None


def get_gemini_service() -> GeminiService:
    """Get the global Gemini service instance."""
    global _gemini_service
    if _gemini_service is None:
        _gemini_service = GeminiService()
    return _gemini_service


def reset_gemini_service():
    """Reset the global Gemini service instance."""
    global _gemini_service
    _gemini_service = None
